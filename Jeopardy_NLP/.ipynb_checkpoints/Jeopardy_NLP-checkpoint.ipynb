{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = 'JEOPARDY_CSV.csv'\n",
    "fulldata = pd.read_csv(directory).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldata = fulldata[fulldata[' Value'] != 'None']\n",
    "fulldata = fulldata[fulldata[' Round'] == 'Jeopardy!']\n",
    "fulldata = fulldata.reset_index(drop=True)\n",
    "\n",
    "count = 0\n",
    "for amount in fulldata[' Value']:\n",
    "    fulldata.at[count, ' Value'] = int(amount.replace('$','').replace(',',''))\n",
    "    count += 1\n",
    "fulldata.head()\n",
    "\n",
    "questiondata = fulldata[[' Value', ' Question', ' Round', ' Answer']].head(5000) #taking just the first 5000 questions because otherwise the dataframe is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questiondata = questiondata[questiondata[' Value'] < 1001]\n",
    "questiondata = questiondata[questiondata[' Value'] != 300]\n",
    "questiondata = questiondata[questiondata[' Value'] != 500]\n",
    "questiondata = questiondata[questiondata[' Value'] != 100]\n",
    "questiondata = questiondata[questiondata[' Value'] != 900]\n",
    "questiondata = questiondata[questiondata[' Value'] != 700] #I looked for a better way to strip out all other values but couldn't find one\n",
    "\n",
    "questiondata = questiondata.reset_index(drop=True) #for some reason the index gets messed up when I do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "count = 0\n",
    "for question in questiondata[' Question']:\n",
    "    question = ''.join(ch for ch in question if ch not in exclude)\n",
    "    questiondata.at[count, ' Question'] = question.lower()\n",
    "    count += 1 #this box just removes puncuation and capitalization so that all words are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questiondata = questiondata[~questiondata[' Question'].str.contains('href')] #removes video questions\n",
    "questiondata = questiondata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "questiondict = {}\n",
    "allwordstring = \"\"\n",
    "\n",
    "count = 0\n",
    "for question in questiondata[' Question']:\n",
    "    questiondict[count] = word_tokenize(question) #splits questions into words\n",
    "    allwordstring += ' ' + question\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = word_tokenize(allwordstring)\n",
    "allwords = list(dict.fromkeys(allwords)) #makes a list of all words (word bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worddict = {}\n",
    "count = 0\n",
    "for word in allwords:\n",
    "    worddict[count] = word #makes a dict of all words for iteration\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate = pd.DataFrame(index=questiondict.keys(), columns=worddict.values()) #empty df for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = []\n",
    "\n",
    "for row, words in iterate.iterrows():\n",
    "    print(row)\n",
    "    countdict = []\n",
    "    for word in words.keys():\n",
    "        countdict.append(questiondict[row].count(word))\n",
    "    alldata.append(countdict) #adding results of counting into a 2 dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(alldata, columns = worddict.values()) #adding 2D array into main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = df.to_csv('ALLDATA.csv') #saving df so that I can start here instead of repeating this entire process every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALLDATA.csv')\n",
    "df = df.drop(['Unnamed: 0'],axis=1) #for some reason it adds an extra index column??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdict = {'num':[], 'error':[]}\n",
    "\n",
    "for num in range(15):\n",
    "    print(num)\n",
    "    if num > 0:\n",
    "        kmeans = KMeans(n_clusters=num)\n",
    "        kmeans.fit(df)\n",
    "        clusterdict['num'].append(num)\n",
    "        clusterdict['error'].append(kmeans.inertia_)\n",
    "\n",
    "df_inertia = pd.DataFrame.from_dict(clusterdict) #this gives the elbow plot but it takes forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot('num', 'error', data = df_inertia, size=5,aspect=1) #graphs the elbow plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(5) #5 is the actual number of values that I have (200, 400, 600, 800, 1000)\n",
    "kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"picklemodel.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(kmeans, file) #saving the model to use in the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = df.values\n",
    "labels = []\n",
    "for row in np_data:\n",
    "    vals = row.reshape(1, -1)\n",
    "    label = kmeans.predict(vals)\n",
    "    label= label[0]\n",
    "    labels.append(label)\n",
    "questiondata['labels'] = labels #adding labels back into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=' Value', hue='labels',palette='rainbow', data=questiondata) #plots the final labels (inconclusive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
